# -*- coding: utf-8 -*-
"""base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I95SXYW1Ypd6IK_l40SfyjKzF3NGMNRZ
"""

import numpy as  np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.data.dataset import random_split
import torch.functional as F
import torch.optim as optim

from sklearn.datasets import make_regression

np.random.seed(42)
X, y = make_regression(n_samples=300, n_features=1, bias=3, n_informative=1, noise=10)
y = y.reshape(-1,1)
X.shape, y.shape

plt.scatter(X,y)

X = torch.as_tensor(X, dtype=torch.float)
y = torch.as_tensor(y, dtype=torch.float)
X.dtype, y.dtype

dataset = TensorDataset(X,y)

total_observation = len(X)
N_train = int(0.7 * total_observation) # 70% training
N_val = total_observation -  N_train
train_data, val_data = random_split(dataset, [N_train, N_val])
len(train_data), len(val_data)

train_loader = DataLoader(train_data, batch_size=8, shuffle=True)
val_loader = DataLoader(val_data, batch_size=8, shuffle=True)

class BaseModel:

  def __init__(self, model, loss_fn, optimizer):

    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self.model = model
    self.loss_fn = loss_fn
    self.optimizer = optimizer

    self.train_loader = None
    self.val_loader = None

    self.losses = []
    self.val_losses = []
    self.epochs = 0

  def set_loaders(self, train_loader, val_loader):
    self.train_loader = train_loader
    self.val_loader = val_loader

  def _training_step(self):

    def train(x, y):
      self.model.train()
      y_pred = self.model(x)
      loss = self.loss_fn(y_pred, y)
      loss.backward()
      self.optimizer.step()
      self.optimizer.zero_grad()
      return loss.item()
    return train

  def _validation_step(self):

    def validate(x, y):
      self.model.eval()
      y_pred = self.model(x)
      loss = self.loss_fn(y_pred, y)
      return loss.item()
    return validate

  def _mini_batch(self, validation=False):

    if validation:
      data_loader = self.val_loader
      step_fn = self._validation_step()
    else:
      data_loader = self.train_loader
      step_fn = self._training_step()

    mini_batch_losses = []
    for x_mini_batch, y_mini_batch in data_loader:
      x_mini_batch = x_mini_batch.to(self.device)
      y_mini_batch = y_mini_batch.to(self.device)

      loss = step_fn(x_mini_batch, y_mini_batch)
      mini_batch_losses.append(loss)
    return np.mean(mini_batch_losses)

  def train(self, epochs):
    for _ in range(epochs):

      loss = self._mini_batch()
      self.losses.append(loss)

      with torch.no_grad():
        val_loss = self._mini_batch(validation=True)
        self.val_losses.append(val_loss)


  def predict(self, X):
    self.model.eval()

    X = torch.as_tensor(X, dtype=torch.float32).to(self.device)
    y_pred = self.model(X)

    self.model.train()

    return y_pred.detach().cpu().numpy()

class LinearModel(nn.Module):

  def __init__(self):
    super().__init__()
    self.fc = nn.Linear(1,1)

  def forward(self, X):
    return self.fc(X)

lr = 0.01
model = LinearModel()

optimizer = optim.SGD(model.parameters(), lr=lr)
loss_fn = nn.MSELoss()


base = BaseModel(model, loss_fn, optimizer)
base.train_loader = train_loader
base.val_loader = val_loader

base.train(200)

pred = base.predict(X)

plt.scatter(X,y)
plt.scatter(X,pred)

plt.plot(range(200), base.losses)
plt.plot(range(200), base.val_losses)

